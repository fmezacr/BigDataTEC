{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro ML SUP en BD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la sesión Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session object\n",
    "\n",
    "spark = SparkSession.builder.appName('supervised_ml').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de datos, archivo *Linear_regression_dataset.csv*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Linear_regression_dataset.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se invocan las librerias correcpondientes a **LinearRegression**, asi como las de OneHotEncoder, StringIndexer, VectorAssembler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan algunos datos: X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1232, 6)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestran los primeros 10 datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+\n",
      "|var_1|var_2|var_3|var_4|var_5|label|\n",
      "+-----+-----+-----+-----+-----+-----+\n",
      "|  734|  688|   81|0.328|0.259|0.418|\n",
      "|  700|  600|   94| 0.32|0.247|0.389|\n",
      "|  712|  705|   93|0.311|0.247|0.417|\n",
      "|  734|  806|   69|0.315| 0.26|0.415|\n",
      "|  613|  759|   61|0.302| 0.24|0.378|\n",
      "|  748|  676|   85|0.318|0.255|0.422|\n",
      "|  669|  588|   97|0.315|0.251|0.411|\n",
      "|  667|  845|   68|0.324|0.251|0.381|\n",
      "|  758|  890|   64| 0.33|0.274|0.436|\n",
      "|  726|  670|   88|0.335|0.268|0.422|\n",
      "+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un solo vector con todos los features i.e 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', a este le llamaremos \"features\" y como salida colocamos a 'label':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['var_1', 'var_2', 'var_3', 'var_4', 'var_5'], outputCol = \"features\")\n",
    "df = df_assembler.transform(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[734.0,688.0,81.0...|0.418|\n",
      "|[700.0,600.0,94.0...|0.389|\n",
      "|[712.0,705.0,93.0...|0.417|\n",
      "|[734.0,806.0,69.0...|0.415|\n",
      "|[613.0,759.0,61.0...|0.378|\n",
      "|[748.0,676.0,85.0...|0.422|\n",
      "|[669.0,588.0,97.0...|0.411|\n",
      "|[667.0,845.0,68.0...|0.381|\n",
      "|[758.0,890.0,64.0...|0.436|\n",
      "|[726.0,670.0,88.0...|0.422|\n",
      "|[583.0,794.0,55.0...|0.371|\n",
      "|[676.0,746.0,72.0...|  0.4|\n",
      "|[767.0,699.0,89.0...|0.433|\n",
      "|[637.0,597.0,86.0...|0.374|\n",
      "|[609.0,724.0,69.0...|0.382|\n",
      "|[776.0,733.0,83.0...|0.437|\n",
      "|[701.0,832.0,66.0...| 0.39|\n",
      "|[650.0,709.0,74.0...|0.386|\n",
      "|[804.0,668.0,95.0...|0.453|\n",
      "|[713.0,614.0,94.0...|0.404|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['features','label']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos a continuación el set de datos en 75% training y 25% testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train Dataset : 927\n",
      "Size of test Dataset : 305\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.75, 0.25])\n",
    "print(f\"Size of train Dataset : {train.count()}\" )\n",
    "print(f\"Size of test Dataset : {test.count()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamamos a la Regresión Lineal: X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo de regresión lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el dataframe de prediciones (*predictions_df*) a partir del modelo de entrenamiento y el conjunto de datos test: X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos el contenido de *predictions_df*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "|var_1|var_2|var_3|var_4|var_5|label|            features|         prediction|\n",
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "|  513|  698|   61|0.298|0.236|0.339|[513.0,698.0,61.0...|0.33080780383118386|\n",
      "|  527|  569|   75|0.297|0.239|0.341|[527.0,569.0,75.0...|0.33394050835997224|\n",
      "|  550|  631|   76|0.306|0.235|0.318|[550.0,631.0,76.0...| 0.3368023685331818|\n",
      "|  552|  683|   71| 0.31|0.244|0.335|[552.0,683.0,71.0...|0.34092306559469227|\n",
      "|  554|  536|   77|0.306| 0.24|0.339|[554.0,536.0,77.0...|0.33666621064803975|\n",
      "|  556|  675|   67|0.292|0.233|0.348|[556.0,675.0,67.0...|0.34810563160275215|\n",
      "|  558|  688|   67|0.298|0.233| 0.35|[558.0,688.0,67.0...| 0.3453557908329167|\n",
      "|  558|  740|   60|0.301| 0.24| 0.36|[558.0,740.0,60.0...| 0.3482180529336537|\n",
      "|  562|  546|   79|0.299|0.237| 0.35|[562.0,546.0,79.0...|0.34334128841584893|\n",
      "|  567|  587|   84|0.301|0.238|0.349|[567.0,587.0,84.0...| 0.3466272661937242|\n",
      "|  569|  544|   82|0.304| 0.24|0.343|[569.0,544.0,82.0...|0.34419997833502614|\n",
      "|  569|  620|   77|0.302|0.247|0.349|[569.0,620.0,77.0...|0.35168938267982547|\n",
      "|  570|  578|   86|0.303|0.242|0.368|[570.0,578.0,86.0...|0.34816297473464597|\n",
      "|  570|  655|   66|0.311|0.246| 0.34|[570.0,655.0,66.0...| 0.3457201799969145|\n",
      "|  571|  577|   83|0.298|0.251|0.368|[571.0,577.0,83.0...|0.35589473355401596|\n",
      "|  573|  634|   75|0.308|0.244|0.342|[573.0,634.0,75.0...| 0.3479611816722346|\n",
      "|  574|  517|   83|0.297|0.231|0.331|[574.0,517.0,83.0...|0.34513821280661827|\n",
      "|  574|  647|   72|0.302|0.244|0.355|[574.0,647.0,72.0...|0.35252775522577895|\n",
      "|  575|  864|   55|  0.3|0.248|0.379|[575.0,864.0,55.0...|0.36356177755574703|\n",
      "|  576|  759|   57|0.313|0.254| 0.35|[576.0,759.0,57.0...| 0.3538598694160282|\n",
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, evaluamos el modelo de Regresión Lineal, con los datos de TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = lr_model.evaluate(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el valor de R2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8667564389069377\n"
     ]
    }
   ],
   "source": [
    "print(model_predictions.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el valor del meanSquaredError:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00014383760230078483\n"
     ]
    }
   ],
   "source": [
    "print(model_predictions.meanSquaredError)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con Árboles de Decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la librería *DecisionTreeRegressor*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el Regresor DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model. \n",
    "dec_tree_model = dec_tree.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuánto es la profundidad máxima por defecto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desplegamos las *featureImportances*: X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.9684, 1: 0.0088, 2: 0.0059, 3: 0.0075, 4: 0.0094})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tree_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el modelo con los datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "model_predictions = dec_tree_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "|var_1|var_2|var_3|var_4|var_5|label|            features|         prediction|\n",
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "|  513|  698|   61|0.298|0.236|0.339|[513.0,698.0,61.0...| 0.3443333333333333|\n",
      "|  527|  569|   75|0.297|0.239|0.341|[527.0,569.0,75.0...|            0.33075|\n",
      "|  550|  631|   76|0.306|0.235|0.318|[550.0,631.0,76.0...| 0.3519880952380952|\n",
      "|  552|  683|   71| 0.31|0.244|0.335|[552.0,683.0,71.0...| 0.3519880952380952|\n",
      "|  554|  536|   77|0.306| 0.24|0.339|[554.0,536.0,77.0...| 0.3519880952380952|\n",
      "|  556|  675|   67|0.292|0.233|0.348|[556.0,675.0,67.0...| 0.3519880952380952|\n",
      "|  558|  688|   67|0.298|0.233| 0.35|[558.0,688.0,67.0...| 0.3519880952380952|\n",
      "|  558|  740|   60|0.301| 0.24| 0.36|[558.0,740.0,60.0...| 0.3519880952380952|\n",
      "|  562|  546|   79|0.299|0.237| 0.35|[562.0,546.0,79.0...| 0.3519880952380952|\n",
      "|  567|  587|   84|0.301|0.238|0.349|[567.0,587.0,84.0...| 0.3519880952380952|\n",
      "|  569|  544|   82|0.304| 0.24|0.343|[569.0,544.0,82.0...| 0.3519880952380952|\n",
      "|  569|  620|   77|0.302|0.247|0.349|[569.0,620.0,77.0...| 0.3519880952380952|\n",
      "|  570|  578|   86|0.303|0.242|0.368|[570.0,578.0,86.0...| 0.3519880952380952|\n",
      "|  570|  655|   66|0.311|0.246| 0.34|[570.0,655.0,66.0...| 0.3519880952380952|\n",
      "|  571|  577|   83|0.298|0.251|0.368|[571.0,577.0,83.0...| 0.3519880952380952|\n",
      "|  573|  634|   75|0.308|0.244|0.342|[573.0,634.0,75.0...| 0.3519880952380952|\n",
      "|  574|  517|   83|0.297|0.231|0.331|[574.0,517.0,83.0...| 0.3519880952380952|\n",
      "|  574|  647|   72|0.302|0.244|0.355|[574.0,647.0,72.0...| 0.3519880952380952|\n",
      "|  575|  864|   55|  0.3|0.248|0.379|[575.0,864.0,55.0...|0.37400000000000055|\n",
      "|  576|  759|   57|0.313|0.254| 0.35|[576.0,759.0,57.0...| 0.3519880952380952|\n",
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el **RegressionEvaluator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando *RegressionEvaluator* calculamos e imprimimos el valor de las metricas R2 y RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r-square value of DecisionTreeRegressor is 0.8205888870252986\n",
      "The rmse value of DecisionTreeRegressor is 0.013916748282450318\n"
     ]
    }
   ],
   "source": [
    "# R2 value of the model on test data \n",
    "dt_evaluator = RegressionEvaluator(metricName='r2')\n",
    "dt_r2 = dt_evaluator.evaluate(model_predictions)\n",
    "print(f'The r-square value of DecisionTreeRegressor is {dt_r2}')\n",
    "\n",
    "# RMSE value of the model on test data \n",
    "dt_evaluator = RegressionEvaluator(metricName='rmse')\n",
    "dt_rmse = dt_evaluator.evaluate(model_predictions)\n",
    "print(f'The rmse value of DecisionTreeRegressor is {dt_rmse}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos a *RandomForestRegressor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el Regresor RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model. \n",
    "rf_model = rf.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desplegamos las *featureImportances*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.4865, 1: 0.0333, 2: 0.0197, 3: 0.2518, 4: 0.2088})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desplegamos el numero de arboles (Num of Trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.getNumTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el modelo con los datos de entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desplegamos los valores del *model_predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "|var_1|var_2|var_3|var_4|var_5|label|            features|         prediction|\n",
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "|  513|  698|   61|0.298|0.236|0.339|[513.0,698.0,61.0...| 0.3397480065197819|\n",
      "|  527|  569|   75|0.297|0.239|0.341|[527.0,569.0,75.0...|0.33863136508008124|\n",
      "|  550|  631|   76|0.306|0.235|0.318|[550.0,631.0,76.0...|   0.35189234524107|\n",
      "|  552|  683|   71| 0.31|0.244|0.335|[552.0,683.0,71.0...| 0.3530088803560934|\n",
      "|  554|  536|   77|0.306| 0.24|0.339|[554.0,536.0,77.0...|   0.35189234524107|\n",
      "|  556|  675|   67|0.292|0.233|0.348|[556.0,675.0,67.0...| 0.3405007821844655|\n",
      "|  558|  688|   67|0.298|0.233| 0.35|[558.0,688.0,67.0...|0.34691693394647594|\n",
      "|  558|  740|   60|0.301| 0.24| 0.36|[558.0,740.0,60.0...| 0.3526783380862997|\n",
      "|  562|  546|   79|0.299|0.237| 0.35|[562.0,546.0,79.0...| 0.3524942133729381|\n",
      "|  567|  587|   84|0.301|0.238|0.349|[567.0,587.0,84.0...|0.35320027169255275|\n",
      "|  569|  544|   82|0.304| 0.24|0.343|[569.0,544.0,82.0...|0.35259840356068467|\n",
      "|  569|  620|   77|0.302|0.247|0.349|[569.0,620.0,77.0...|0.35213622413931095|\n",
      "|  570|  578|   86|0.303|0.242|0.368|[570.0,578.0,86.0...|0.35339627438598004|\n",
      "|  570|  655|   66|0.311|0.246| 0.34|[570.0,655.0,66.0...|0.35249346936322284|\n",
      "|  571|  577|   83|0.298|0.251|0.368|[571.0,577.0,83.0...|  0.354940341664426|\n",
      "|  573|  634|   75|0.308|0.244|0.342|[573.0,634.0,75.0...|  0.352805745457499|\n",
      "|  574|  517|   83|0.297|0.231|0.331|[574.0,517.0,83.0...| 0.3463500859114389|\n",
      "|  574|  647|   72|0.302|0.244|0.355|[574.0,647.0,72.0...|0.35155541661967005|\n",
      "|  575|  864|   55|  0.3|0.248|0.379|[575.0,864.0,55.0...|0.35765441595673975|\n",
      "|  576|  759|   57|0.313|0.254| 0.35|[576.0,759.0,57.0...|0.36110030212833377|\n",
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando *RegressionEvaluator* calculamos e imprimimos el valor de las metricas R2 y RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r-square value of RandomForestRegressor is 0.830007622223091\n",
      "The rmse value of RandomForestRegressor is 0.013546522610373079\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "# R2 value of the model on test data \n",
    "rf_evaluator = RegressionEvaluator(metricName='r2')\n",
    "rf_r2 = rf_evaluator.evaluate(model_predictions)\n",
    "print(f'The r-square value of RandomForestRegressor is {rf_r2}')\n",
    "\n",
    "# RMSE value of the model on test data \n",
    "rf_evaluator = RegressionEvaluator(metricName='rmse')\n",
    "rf_rmse = rf_evaluator.evaluate(model_predictions)\n",
    "print(f'The rmse value of RandomForestRegressor is {rf_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos a GBTRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el Regresor GBTR:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  \n",
    "gbt_model = gbt.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desplegamos las featureImportances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.2353, 1: 0.2351, 2: 0.1922, 3: 0.1754, 4: 0.1621})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el modelo con los datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = gbt_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desplegamos los valores del *model_predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "|var_1|var_2|var_3|var_4|var_5|label|            features|         prediction|\n",
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "|  513|  698|   61|0.298|0.236|0.339|[513.0,698.0,61.0...| 0.3408141425096789|\n",
      "|  527|  569|   75|0.297|0.239|0.341|[527.0,569.0,75.0...| 0.3349061901475704|\n",
      "|  550|  631|   76|0.306|0.235|0.318|[550.0,631.0,76.0...|0.34619106623178414|\n",
      "|  552|  683|   71| 0.31|0.244|0.335|[552.0,683.0,71.0...| 0.3468438238014841|\n",
      "|  554|  536|   77|0.306| 0.24|0.339|[554.0,536.0,77.0...|   0.34967715751417|\n",
      "|  556|  675|   67|0.292|0.233|0.348|[556.0,675.0,67.0...|0.34832111114385683|\n",
      "|  558|  688|   67|0.298|0.233| 0.35|[558.0,688.0,67.0...| 0.3484689044144408|\n",
      "|  558|  740|   60|0.301| 0.24| 0.36|[558.0,740.0,60.0...| 0.3503177083105877|\n",
      "|  562|  546|   79|0.299|0.237| 0.35|[562.0,546.0,79.0...|   0.34967715751417|\n",
      "|  567|  587|   84|0.301|0.238|0.349|[567.0,587.0,84.0...| 0.3469696177472521|\n",
      "|  569|  544|   82|0.304| 0.24|0.343|[569.0,544.0,82.0...|0.35336106184652055|\n",
      "|  569|  620|   77|0.302|0.247|0.349|[569.0,620.0,77.0...| 0.3556837692485369|\n",
      "|  570|  578|   86|0.303|0.242|0.368|[570.0,578.0,86.0...| 0.3594751146046992|\n",
      "|  570|  655|   66|0.311|0.246| 0.34|[570.0,655.0,66.0...| 0.3458122810092967|\n",
      "|  571|  577|   83|0.298|0.251|0.368|[571.0,577.0,83.0...| 0.3599836235594222|\n",
      "|  573|  634|   75|0.308|0.244|0.342|[573.0,634.0,75.0...| 0.3429403277909437|\n",
      "|  574|  517|   83|0.297|0.231|0.331|[574.0,517.0,83.0...|0.35336106184652055|\n",
      "|  574|  647|   72|0.302|0.244|0.355|[574.0,647.0,72.0...| 0.3442008883991257|\n",
      "|  575|  864|   55|  0.3|0.248|0.379|[575.0,864.0,55.0...| 0.3716049218670243|\n",
      "|  576|  759|   57|0.313|0.254| 0.35|[576.0,759.0,57.0...| 0.3525310705255906|\n",
      "+-----+-----+-----+-----+-----+-----+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando RegressionEvaluator calculamos e imprimimos el valor de las metricas R2 y RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r-square value of GradientBoostedRegressor is 0.8205459288992225\n",
      "The rmse value of GradientBoostedRegressor is 0.013918414293086918\n"
     ]
    }
   ],
   "source": [
    " #Select (prediction, true label) and compute test error\n",
    "# R2 value of the model on test data \n",
    "gbt_evaluator = RegressionEvaluator(metricName='r2')\n",
    "gbt_r2 = gbt_evaluator.evaluate(model_predictions)\n",
    "print(f'The r-square value of GradientBoostedRegressor is {gbt_r2}')\n",
    "\n",
    "# RMSE value of the model on test data \n",
    "gbt_evaluator = RegressionEvaluator(metricName='rmse')\n",
    "gbt_rmse = gbt_evaluator.evaluate(model_predictions)\n",
    "print(f'The rmse value of GradientBoostedRegressor is {gbt_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exploracion de datos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el dataset https://archive.ics.uci.edu/ml/datasets/Bank+Marketing \n",
    "\n",
    "Indique a grandes razgos de que se trata este dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de datos, archivo bank_data.csv:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv Dataset \n",
    "df=spark.read.csv('bank_data.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine la cantidad de datos en el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41188"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A que dato corresponde cada columna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'month',\n",
       " 'day_of_week',\n",
       " 'duration',\n",
       " 'campaign',\n",
       " 'pdays',\n",
       " 'previous',\n",
       " 'poutcome',\n",
       " 'emp.var.rate',\n",
       " 'cons.price.idx',\n",
       " 'cons.conf.idx',\n",
       " 'euribor3m',\n",
       " 'nr.employed',\n",
       " 'target_class']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprima el Schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp.var.rate: double (nullable = true)\n",
      " |-- cons.price.idx: double (nullable = true)\n",
      " |-- cons.conf.idx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nr.employed: double (nullable = true)\n",
      " |-- target_class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataype of input data \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la salida, como es la distrubución de clases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|target_class|count|\n",
      "+------------+-----+\n",
      "|          no|36548|\n",
      "|         yes| 4640|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('target_class').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una tarea típica, resulta de convertir los valores binarios en 1 y 0, usando como referencia \"label\", convierta los no/yes en 0/1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingrese acá la instrucción: \n",
    "\n",
    "df = df.withColumn(\"label\", F.when(df.target_class =='no', F.lit(0)).otherwise(F.lit(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 4640|\n",
      "|    0|36548|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, StringType\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la sesion SPARK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('deep_learning').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('dl_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Visit_Number_Bucket: string (nullable = true)\n",
      " |-- Page_Views_Normalized: double (nullable = true)\n",
      " |-- Orders_Normalized: integer (nullable = true)\n",
      " |-- Internal_Search_Successful_Normalized: double (nullable = true)\n",
      " |-- Internal_Search_Null_Normalized: double (nullable = true)\n",
      " |-- Email_Signup_Normalized: double (nullable = true)\n",
      " |-- Total_Seconds_Spent_Normalized: double (nullable = true)\n",
      " |-- Store_Locator_Search_Normalized: double (nullable = true)\n",
      " |-- Mapped_Last_Touch_Channel: string (nullable = true)\n",
      " |-- Mapped_Mobile_Device_Type: string (nullable = true)\n",
      " |-- Mapped_Browser_Type: string (nullable = true)\n",
      " |-- Mapped_Entry_Pages: string (nullable = true)\n",
      " |-- Mapped_Site_Section: string (nullable = true)\n",
      " |-- Mapped_Promo_Code: string (nullable = true)\n",
      " |-- Maped_Product_Name: string (nullable = true)\n",
      " |-- Mapped_Search_Term: string (nullable = true)\n",
      " |-- Mapped_Product_Collection: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renombramos la columna TARGET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed('Orders_Normalized', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Visit_Number_Bucket: string (nullable = true)\n",
      " |-- Page_Views_Normalized: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- Internal_Search_Successful_Normalized: double (nullable = true)\n",
      " |-- Internal_Search_Null_Normalized: double (nullable = true)\n",
      " |-- Email_Signup_Normalized: double (nullable = true)\n",
      " |-- Total_Seconds_Spent_Normalized: double (nullable = true)\n",
      " |-- Store_Locator_Search_Normalized: double (nullable = true)\n",
      " |-- Mapped_Last_Touch_Channel: string (nullable = true)\n",
      " |-- Mapped_Mobile_Device_Type: string (nullable = true)\n",
      " |-- Mapped_Browser_Type: string (nullable = true)\n",
      " |-- Mapped_Entry_Pages: string (nullable = true)\n",
      " |-- Mapped_Site_Section: string (nullable = true)\n",
      " |-- Mapped_Promo_Code: string (nullable = true)\n",
      " |-- Maped_Product_Name: string (nullable = true)\n",
      " |-- Mapped_Search_Term: string (nullable = true)\n",
      " |-- Mapped_Product_Collection: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos lo datos en Train, Validation y Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test  = data.randomSplit([0.7, 0.2, 0.1], 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [item[0] for item in data.dtypes if item[1].startswith('string')]\n",
    "numeric_columns = [item[0] for item in data.dtypes if item[1].startswith('double')]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(column)) for column in categorical_columns]\n",
    "\n",
    "featuresCreator = VectorAssembler(inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns, outputCol=\"features\")\n",
    "\n",
    "layers = [len(featuresCreator.getInputCols()), 4, 2, 2]\n",
    "\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='label', featuresCol='features', maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [featuresCreator, classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos y Evaluamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_df = model.transform(train)\n",
    "validation_output_df = model.transform(validation)\n",
    "test_output_df = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llevamos a cabo, algunas predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train weightedPrecision = 0.9678672585062094\n",
      "Validation weightedPrecision = 0.9691383210897103\n",
      "Test weightedPrecision = 0.9694943540959478\n",
      "Train weightedRecall = 0.967375637879319\n",
      "Validation weightedRecall = 0.9685636856368564\n",
      "Test weightedRecall = 0.9691266079891672\n",
      "Train accuracy = 0.9673756378793191\n",
      "Validation accuracy = 0.9685636856368564\n",
      "Test accuracy = 0.9691266079891673\n"
     ]
    }
   ],
   "source": [
    "train_predictionAndLabels = train_output_df.select(\"prediction\", \"label\")\n",
    "validation_predictionAndLabels = validation_output_df.select(\"prediction\", \"label\")\n",
    "test_predictionAndLabels = test_output_df.select(\"prediction\", \"label\")\n",
    "\n",
    "metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
    "\n",
    "for metric in metrics:\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
    "    print('Train ' + metric + ' = ' + str(evaluator.evaluate(train_predictionAndLabels)))\n",
    "    print('Validation ' + metric + ' = ' + str(evaluator.evaluate(validation_predictionAndLabels)))\n",
    "    print('Test ' + metric + ' = ' + str(evaluator.evaluate(test_predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede mejorar el test accuracy del modelo variando alguno de los hyperparametros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
